# -*- coding: utf-8 -*-
"""
å‘½ä»¤è¡Œè¯„æµ‹å·¥å…·

è‡ªåŠ¨è¯„æµ‹é¢æ–™è¯†åˆ«å‡†ç¡®ç‡å’Œæ€§èƒ½

ç”¨æ³•:
    python tools/eval_cli.py --dir eval_set
    python tools/eval_cli.py --dir eval_set --top-k 5
    python tools/eval_cli.py --dir eval_set --output logs/my_eval.csv
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import time
from typing import List, Dict, Tuple
import csv
from datetime import datetime
from PIL import Image
import numpy as np
from tqdm import tqdm

from src.core.recommender import recommend
from src.utils.logger import get_logger

log = get_logger("eval_cli")


def load_eval_dataset(eval_dir: Path) -> Dict[str, List[Path]]:
    """
    åŠ è½½è¯„æµ‹æ•°æ®é›†
    
    ç›®å½•ç»“æ„ï¼š
        eval_dir/
        â”œâ”€â”€ cotton/
        â”‚   â”œâ”€â”€ img1.jpg
        â”‚   â”œâ”€â”€ img2.jpg
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ linen/
        â”‚   â””â”€â”€ ...
        â””â”€â”€ ...
    
    Args:
        eval_dir: è¯„æµ‹æ•°æ®é›†ç›®å½•
    
    Returns:
        {label: [image_paths]}
    """
    if not eval_dir.exists():
        log.error(f"è¯„æµ‹ç›®å½•ä¸å­˜åœ¨: {eval_dir}")
        raise FileNotFoundError(f"è¯„æµ‹ç›®å½•ä¸å­˜åœ¨: {eval_dir}")
    
    dataset = {}
    
    # éå†æ¯ä¸ªç±»åˆ«ç›®å½•
    for label_dir in eval_dir.iterdir():
        if not label_dir.is_dir():
            continue
        
        label = label_dir.name
        
        # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾ç‰‡
        images = []
        for ext in ['.jpg', '.jpeg', '.png', '.webp']:
            images.extend(label_dir.glob(f"*{ext}"))
            images.extend(label_dir.glob(f"*{ext.upper()}"))
        
        if images:
            dataset[label] = sorted(images)
            log.info(f"ç±»åˆ« {label}: {len(images)} å¼ å›¾ç‰‡")
    
    if not dataset:
        log.error(f"æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡: {eval_dir}")
        raise ValueError(f"æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡: {eval_dir}")
    
    total_images = sum(len(imgs) for imgs in dataset.values())
    log.info(f"æ€»è®¡: {len(dataset)} ä¸ªç±»åˆ«, {total_images} å¼ å›¾ç‰‡")
    
    return dataset


def evaluate_image(
    image_path: Path,
    ground_truth: str,
    top_k: int = 5
) -> Tuple[Dict, float]:
    """
    è¯„æµ‹å•å¼ å›¾ç‰‡
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        ground_truth: çœŸå®æ ‡ç­¾
        top_k: è¿”å›å‰ K ä¸ªç»“æœ
    
    Returns:
        result_dict: è¯„æµ‹ç»“æœå­—å…¸
        elapsed_ms: è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
    """
    try:
        # åŠ è½½å›¾ç‰‡
        img = Image.open(image_path).convert("RGB")
        
        # æ¨è
        t0 = time.perf_counter()
        result, meta = recommend(img, top_k=top_k, lang="en")
        elapsed_ms = (time.perf_counter() - t0) * 1000
        
        # æå–é¢„æµ‹ç»“æœ
        predictions = [item.label for item in result.items]
        scores = [item.score for item in result.items]
        
        # åˆ¤æ–­å‡†ç¡®æ€§
        top1_correct = predictions[0] == ground_truth if predictions else False
        top3_correct = ground_truth in predictions[:3] if len(predictions) >= 3 else False
        top5_correct = ground_truth in predictions[:5] if len(predictions) >= 5 else False
        
        result_dict = {
            'image': image_path.name,
            'ground_truth': ground_truth,
            'top1_pred': predictions[0] if predictions else 'N/A',
            'top1_score': scores[0] if scores else 0.0,
            'top3_preds': ','.join(predictions[:3]),
            'top5_preds': ','.join(predictions[:5]),
            'top1_correct': top1_correct,
            'top3_correct': top3_correct,
            'top5_correct': top5_correct,
            'time_ms': elapsed_ms,
            'coarse_max': meta.coarse_max,
            'ai_reason': result.ai_reason
        }
        
        return result_dict, elapsed_ms
        
    except Exception as e:
        log.error(f"è¯„æµ‹å¤±è´¥ {image_path}: {e}")
        return {
            'image': image_path.name,
            'ground_truth': ground_truth,
            'top1_pred': 'ERROR',
            'top1_score': 0.0,
            'top3_preds': '',
            'top5_preds': '',
            'top1_correct': False,
            'top3_correct': False,
            'top5_correct': False,
            'time_ms': 0.0,
            'coarse_max': 0.0,
            'ai_reason': str(e)
        }, 0.0


def evaluate_dataset(
    dataset: Dict[str, List[Path]],
    top_k: int = 5,
    output_csv: Path = None
) -> Dict:
    """
    è¯„æµ‹æ•´ä¸ªæ•°æ®é›†
    
    Args:
        dataset: {label: [image_paths]}
        top_k: è¿”å›å‰ K ä¸ªç»“æœ
        output_csv: è¾“å‡º CSV è·¯å¾„
    
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    all_results = []
    all_times = []
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    class_stats = {label: {'total': 0, 'top1': 0, 'top3': 0, 'top5': 0} 
                   for label in dataset.keys()}
    
    # è®¡ç®—æ€»å›¾ç‰‡æ•°
    total_images = sum(len(imgs) for imgs in dataset.values())
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(total=total_images, desc="è¯„æµ‹è¿›åº¦", unit="img")
    
    # éå†æ¯ä¸ªç±»åˆ«
    for label, image_paths in dataset.items():
        log.info(f"\nè¯„æµ‹ç±»åˆ«: {label} ({len(image_paths)} å¼ )")
        
        for img_path in image_paths:
            result_dict, elapsed_ms = evaluate_image(img_path, label, top_k)
            
            all_results.append(result_dict)
            all_times.append(elapsed_ms)
            
            # æ›´æ–°ç»Ÿè®¡
            class_stats[label]['total'] += 1
            if result_dict['top1_correct']:
                class_stats[label]['top1'] += 1
            if result_dict['top3_correct']:
                class_stats[label]['top3'] += 1
            if result_dict['top5_correct']:
                class_stats[label]['top5'] += 1
            
            pbar.update(1)
    
    pbar.close()
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ° CSV
    if output_csv:
        output_csv.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = [
                'image', 'ground_truth', 'top1_pred', 'top1_score',
                'top3_preds', 'top5_preds', 'top1_correct', 'top3_correct',
                'top5_correct', 'time_ms', 'coarse_max', 'ai_reason'
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_results)
        
        log.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_csv}")
    
    # è®¡ç®—æ•´ä½“ç»Ÿè®¡
    total_correct_top1 = sum(1 for r in all_results if r['top1_correct'])
    total_correct_top3 = sum(1 for r in all_results if r['top3_correct'])
    total_correct_top5 = sum(1 for r in all_results if r['top5_correct'])
    
    overall_stats = {
        'total_images': len(all_results),
        'total_classes': len(dataset),
        'top1_accuracy': total_correct_top1 / len(all_results) if all_results else 0,
        'top3_accuracy': total_correct_top3 / len(all_results) if all_results else 0,
        'top5_accuracy': total_correct_top5 / len(all_results) if all_results else 0,
        'class_stats': class_stats,
        'time_stats': {
            'mean': np.mean(all_times) if all_times else 0,
            'median': np.median(all_times) if all_times else 0,
            'p50': np.percentile(all_times, 50) if all_times else 0,
            'p95': np.percentile(all_times, 95) if all_times else 0,
            'p99': np.percentile(all_times, 99) if all_times else 0,
            'min': np.min(all_times) if all_times else 0,
            'max': np.max(all_times) if all_times else 0,
        }
    }
    
    return overall_stats


def print_report(stats: Dict):
    """æ‰“å°è¯„æµ‹æŠ¥å‘Š"""
    
    print("\n" + "=" * 70)
    print("è¯„æµ‹æŠ¥å‘Š")
    print("=" * 70)
    
    # æ•´ä½“å‡†ç¡®ç‡
    print("\nğŸ“Š æ•´ä½“å‡†ç¡®ç‡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
    print(f"  æ€»ç±»åˆ«æ•°: {stats['total_classes']}")
    print(f"  Top-1 å‡†ç¡®ç‡: {stats['top1_accuracy']:.2%} ({int(stats['top1_accuracy'] * stats['total_images'])}/{stats['total_images']})")
    print(f"  Top-3 å‡†ç¡®ç‡: {stats['top3_accuracy']:.2%} ({int(stats['top3_accuracy'] * stats['total_images'])}/{stats['total_images']})")
    print(f"  Top-5 å‡†ç¡®ç‡: {stats['top5_accuracy']:.2%} ({int(stats['top5_accuracy'] * stats['total_images'])}/{stats['total_images']})")
    
    # æŒ‰ç±»åˆ«å‡†ç¡®ç‡
    print("\nğŸ“‹ æŒ‰ç±»åˆ«å‡†ç¡®ç‡:")
    print(f"{'ç±»åˆ«':<20} {'æ€»æ•°':>6} {'Top-1':>8} {'Top-3':>8} {'Top-5':>8}")
    print("-" * 70)
    
    for label, class_stat in sorted(stats['class_stats'].items()):
        total = class_stat['total']
        top1 = class_stat['top1']
        top3 = class_stat['top3']
        top5 = class_stat['top5']
        
        top1_acc = top1 / total if total > 0 else 0
        top3_acc = top3 / total if total > 0 else 0
        top5_acc = top5 / total if total > 0 else 0
        
        print(f"{label:<20} {total:>6} {top1_acc:>7.1%} {top3_acc:>7.1%} {top5_acc:>7.1%}")
    
    # è€—æ—¶ç»Ÿè®¡
    print("\nâ±ï¸  è€—æ—¶ç»Ÿè®¡ (ms):")
    time_stats = stats['time_stats']
    print(f"  å¹³å‡å€¼: {time_stats['mean']:.1f} ms")
    print(f"  ä¸­ä½æ•°: {time_stats['median']:.1f} ms")
    print(f"  P50: {time_stats['p50']:.1f} ms")
    print(f"  P95: {time_stats['p95']:.1f} ms")
    print(f"  P99: {time_stats['p99']:.1f} ms")
    print(f"  æœ€å°å€¼: {time_stats['min']:.1f} ms")
    print(f"  æœ€å¤§å€¼: {time_stats['max']:.1f} ms")
    
    # æ€§èƒ½è¯„ä¼°
    print("\nğŸ¯ æ€§èƒ½è¯„ä¼°:")
    if time_stats['p95'] < 500:
        print("  âœ… ä¼˜ç§€ - P95 < 500ms")
    elif time_stats['p95'] < 1000:
        print("  âœ“ è‰¯å¥½ - P95 < 1000ms")
    else:
        print("  âš ï¸  éœ€è¦ä¼˜åŒ– - P95 >= 1000ms")
    
    print("\n" + "=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="é¢æ–™è¯†åˆ«è¯„æµ‹å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python tools/eval_cli.py --dir eval_set
  python tools/eval_cli.py --dir eval_set --top-k 5
  python tools/eval_cli.py --dir eval_set --output logs/my_eval.csv
        """
    )
    
    parser.add_argument(
        '--dir',
        type=str,
        required=True,
        help='è¯„æµ‹æ•°æ®é›†ç›®å½•ï¼ˆç»“æ„ï¼šdir/<label>/*.jpgï¼‰'
    )
    
    parser.add_argument(
        '--top-k',
        type=int,
        default=5,
        help='è¿”å›å‰ K ä¸ªç»“æœï¼ˆé»˜è®¤ 5ï¼‰'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='logs/eval_report.csv',
        help='è¾“å‡º CSV è·¯å¾„ï¼ˆé»˜è®¤ logs/eval_report.csvï¼‰'
    )
    
    args = parser.parse_args()
    
    # è½¬æ¢è·¯å¾„
    eval_dir = Path(args.dir)
    output_csv = Path(args.output)
    
    print("=" * 70)
    print("é¢æ–™è¯†åˆ«è¯„æµ‹å·¥å…·")
    print("=" * 70)
    print(f"\né…ç½®:")
    print(f"  è¯„æµ‹ç›®å½•: {eval_dir}")
    print(f"  Top-K: {args.top_k}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {output_csv}")
    print(f"  å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        print("\n[1/3] åŠ è½½æ•°æ®é›†...")
        dataset = load_eval_dataset(eval_dir)
        
        # 2. è¯„æµ‹
        print("\n[2/3] å¼€å§‹è¯„æµ‹...")
        stats = evaluate_dataset(dataset, top_k=args.top_k, output_csv=output_csv)
        
        # 3. æ‰“å°æŠ¥å‘Š
        print("\n[3/3] ç”ŸæˆæŠ¥å‘Š...")
        print_report(stats)
        
        print(f"\nâœ… è¯„æµ‹å®Œæˆï¼")
        print(f"è¯¦ç»†ç»“æœ: {output_csv.resolve()}")
        
        return 0
        
    except Exception as e:
        log.exception("è¯„æµ‹å¤±è´¥")
        print(f"\nâŒ è¯„æµ‹å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

